### 05.09 - Введение

Препода зовут Султанов Владимир Захарович, попросил обращаться на "ты"
Занимаемся NLP, ML и чем-то ещё

Не все термины по матеше мы уже прошли, но по ходу курса мы будем их юзать(

Маш обуч - некая тема, которая обобщает множество алгоритмов, которые обобщаются тем что они самообучаются

Большинство алгоритмов, которые мы изучим, редко используют в продакшене, но знание которых не помешает

#### Лекция 1: Статистические методы машинного обучения

- **Типы переменных:**
	- количественные - данные, которые выраж на числовой шкале (непрерывные - любое значение в интервале; дискретные - только целочисл)
	- номинальные - данные, которые принимают только конкретное множество значений (двоичные - всего две категории; порядковые - имеют явно заданное упорядочение); иногда выносят ранговые переменные
	- ранжирующие - которые можно сравнивать, но нельзя проводить над ними мат.операции

- **Виды показателей:**
	- Среднее - сумма всех значений, деленая на число значений
	- Мода - самое часто встречающееся значение
	- Медиана - такое значение, что половина сорт. данных находится выше и ниже данного значения

**Дисперсия** - сумма квадратичных отклонений от среднего, делённая на n-1 где n - число значений данных. Так же иногда называют среднеквадратичной
**Стандартное отклонение** - квадр корень из дисперсии

Дисперсионный анализ (ANOVA)
Степень влияния: $SSB/SST$
Корреляционный анализ - метод математической статистики, используемый для изучения, исследования взаимосвязи между (генеральными) экономическими показателями на основе их наблюдаемых статистических (выборочных) аналогов. При этом сами показатели считаются случайными величинами. 

Коварицией двух случайных величин называется математическое ожидание произведения отклонений этих величин от своих математических ожиданий

Регрессия с одной независимой переменной
$Y = aX + b$ 
$a = \frac{sd_y}{sd_x} * r_{xy}$
$b = Y - a*X$ 
<sup>над иксом надчёркивание должно быть(</sup>
где sd -стандартное отклонение
r - коэффициент корреляции

#### Линейная регрессия

Регрессия и классификация
![[regr_classif.jpg|500]]

#### Точный аналитический метод

$Y = w * X$
$X^T * Y = X^T * (w * X)$
$w = (X^TX)^{-1} X^Ty$

#### Приближённый численный метод
![[approx_numer_meth.jpg|500]]

Расшивровка нижнего кодбока: 
```
w = random_normal() # Можно пробовать и другие виды инициализации
repeat S times: # Другой вариант : while abs(err)> tolerance
	f = X.dot(w) # Посчитать предсказание
	err = f- y # Посчитать ошибку
	grad = 2 * X.T.dot(err) / N # Посчитать градиент
	W -= alpha * grad # Обновить веса
```
Градиент - вектор частных производных #important

#### Gradient descent
![[grad_desc.jpg]]

#### Мультиколлинеарность
телефон зырь #todo

#### Результат линейной зависимости данных
телефон зырь #todo

Сигмоидальная функция: $todo$


### 06.10 - Алгоритм К ближайших соседей

Метрики:
![[knn_metrics.jpg]]
![[knn_metrics_formulas.jpg]]

Метрика косинусного расстояния популярнее всего в NLP

короче, вся инфа в телефоне - законспектируй пж, не забудь( #todo 
K-d-деревья - это процесс размежения точек, отвечающих за положение классов в пространстве, на группы с минимальной энтропией (энтропия - шанс вытащить не тот класс, который тебе нужен, равняется нулю в идеально упорядоченном множестве) #todo
Random projection trees
функция soft max (или softmax) #todo

### 11.10.23
Градиентный бустинг
Случайный лес, по иронии судьбы, показывает наибольшую точность
[[по поводу линейной регрессии]]
### 20.10.23
основной конспект в тетради - тут я просто немного повеселился с [[Excalidraw]]
![[свёртки.excalidraw|700]]
Процесс обучения:
- обучаемый элемент - фильтр
	- $g_1(x) = w_1x + w_0$ 
	- ![[устройство фильтра]]